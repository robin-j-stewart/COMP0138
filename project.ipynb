{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "# Import the FNO2d model from the neural operator library.\n",
    "# (Adjust the import if necessary.)\n",
    "from neuralop.models import FNO2d, TFNO2d, UNO, LocalFNO, FNO3d, TFNO3d\n",
    "from spatioTemporalFNO import SpatioTemporalFNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width: 1786 Height: 966\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00000.jpg\"\n",
    "\n",
    "# Open the image\n",
    "img = Image.open(image_path)\n",
    "\n",
    "# Get its width and height\n",
    "width, height = img.size\n",
    "print(\"Width:\", width, \"Height:\", height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_operator = \"fno\"\n",
    "sequence_length = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSV file and write a header.\n",
    "if sequence_length == 1:\n",
    "    csv_file = \"training_history_\"+neural_operator+\"_bull2.csv\"\n",
    "else:\n",
    "    csv_file = \"training_history_\"+neural_operator+\"_bull2_\"+str(sequence_length)+\".csv\"\n",
    "with open(csv_file, mode=\"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Epoch\", \"MSE Loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class OpticalFlowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads optical flow images for forecasting.\n",
    "    \n",
    "    This dataset can operate in two modes:\n",
    "      - Original Mode (single-frame input): when sequence_length == 1.\n",
    "        Returns:\n",
    "           input  = optical flow at time t (shape: (C, H, W))\n",
    "           target = optical flow at time t + forecast_offset (shape: (C, H, W))\n",
    "      - Sequence Mode (multi-frame input): when sequence_length > 1.\n",
    "        Returns:\n",
    "           input  = sequence of optical flow frames of length `sequence_length` \n",
    "                    (shape: (T, C, H, W))\n",
    "           target = optical flow at time t + sequence_length + forecast_offset - 1\n",
    "                    (shape: (C, H, W))\n",
    "                    \n",
    "    The dataset length is computed appropriately based on the mode.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, transform=None, sequence_length=1, forecast_offset=1):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.sequence_length = sequence_length  # Set to 1 for original behavior; >1 for multi-frame input.\n",
    "        self.forecast_offset = forecast_offset\n",
    "        \n",
    "        self.samples = []\n",
    "        flow_x_files = sorted(glob.glob(os.path.join(data_dir, 'flow_x_*.jpg')))\n",
    "        print(\"Found flow_x files:\", flow_x_files)\n",
    "        for fx in flow_x_files:\n",
    "            fy = fx.replace(\"flow_x_\", \"flow_y_\")\n",
    "            if os.path.exists(fy):\n",
    "                self.samples.append((fx, fy))\n",
    "            else:\n",
    "                print(f\"Warning: Matching flow_y image not found for {fx}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.sequence_length == 1:\n",
    "            # Original: need one frame for input plus forecast_offset for target.\n",
    "            return len(self.samples) - self.forecast_offset\n",
    "        else:\n",
    "            # Sequence mode: need sequence_length frames for input plus forecast_offset.\n",
    "            return len(self.samples) - (self.sequence_length - 1 + self.forecast_offset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.sequence_length == 1:\n",
    "            # ----- Original Mode -----\n",
    "            # Input frame at time t\n",
    "            flow_x_file, flow_y_file = self.samples[idx]\n",
    "            input_flow_x = np.array(Image.open(flow_x_file).convert('L'), dtype=np.float32) / 255.0\n",
    "            input_flow_y = np.array(Image.open(flow_y_file).convert('L'), dtype=np.float32) / 255.0\n",
    "            input_flow = np.stack([input_flow_x, input_flow_y], axis=-1)  # (H, W, 2)\n",
    "            \n",
    "            # Target frame at time t + forecast_offset\n",
    "            target_flow_x_file, target_flow_y_file = self.samples[idx + self.forecast_offset]\n",
    "            target_flow_x = np.array(Image.open(target_flow_x_file).convert('L'), dtype=np.float32) / 255.0\n",
    "            target_flow_y = np.array(Image.open(target_flow_y_file).convert('L'), dtype=np.float32) / 255.0\n",
    "            target_flow = np.stack([target_flow_x, target_flow_y], axis=-1)\n",
    "            \n",
    "            if self.transform is not None:\n",
    "                input_flow = self.transform(input_flow)\n",
    "                target_flow = self.transform(target_flow)\n",
    "            \n",
    "            # Convert to tensor and change order to (C, H, W)\n",
    "            input_tensor = torch.tensor(input_flow, dtype=torch.float32).permute(2, 0, 1)\n",
    "            target_tensor = torch.tensor(target_flow, dtype=torch.float32).permute(2, 0, 1)\n",
    "            \n",
    "            return input_tensor, target_tensor\n",
    "        \n",
    "        else:\n",
    "            # ----- Sequence Mode -----\n",
    "            # Build the input sequence from idx to idx + sequence_length - 1\n",
    "            input_frames = []\n",
    "            for i in range(self.sequence_length):\n",
    "                flow_x_file, flow_y_file = self.samples[idx + i]\n",
    "                input_flow_x = np.array(Image.open(flow_x_file).convert('L'), dtype=np.float32) / 255.0\n",
    "                input_flow_y = np.array(Image.open(flow_y_file).convert('L'), dtype=np.float32) / 255.0\n",
    "                flow = np.stack([input_flow_x, input_flow_y], axis=-1)  # (H, W, 2)\n",
    "                if self.transform is not None:\n",
    "                    flow = self.transform(flow)\n",
    "                flow_tensor = torch.tensor(flow, dtype=torch.float32).permute(2, 0, 1)\n",
    "                input_frames.append(flow_tensor)\n",
    "            # Stack the sequence into a tensor: (T, C, H, W)\n",
    "            input_tensor = torch.stack(input_frames, dim=0)\n",
    "            \n",
    "            # Target frame: located at index = idx + sequence_length + forecast_offset - 1\n",
    "            target_idx = idx + self.sequence_length + self.forecast_offset - 1\n",
    "            target_flow_x_file, target_flow_y_file = self.samples[target_idx]\n",
    "            target_flow_x = np.array(Image.open(target_flow_x_file).convert('L'), dtype=np.float32) / 255.0\n",
    "            target_flow_y = np.array(Image.open(target_flow_y_file).convert('L'), dtype=np.float32) / 255.0\n",
    "            target_flow = np.stack([target_flow_x, target_flow_y], axis=-1)\n",
    "            if self.transform is not None:\n",
    "                target_flow = self.transform(target_flow)\n",
    "            target_tensor = torch.tensor(target_flow, dtype=torch.float32).permute(2, 0, 1)\n",
    "            \n",
    "            return input_tensor, target_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to function call (2928894085.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 17\u001b[0;36m\u001b[0m\n\u001b[0;31m    train_dataset, test_dataset = random_split(dataset, [train_size, test_size]), generator=torch.Generator().manual_seed(seed)\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to function call\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###########################################################################\n",
    "# 2. Prepare the Data (Train/Test Split)\n",
    "###########################################################################\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "data_dir = '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/'  # Replace with the correct path\n",
    "\n",
    "# Create dataset with forecast_offset=1 (predicting next frame)\n",
    "dataset = OpticalFlowDataset(data_dir, forecast_offset=1, sequence_length=sequence_length)\n",
    "print(\"Total available samples for forecasting:\", len(dataset))  # Should be total_frames - 1\n",
    "\n",
    "# Randomly split the dataset into training and testing\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Test dataset size:\", len(test_dataset))\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 1  # Adjust as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found flow_x files: ['/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00000.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00001.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00002.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00003.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00004.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00005.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00006.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00007.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00008.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00009.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00010.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00011.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00012.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00013.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00014.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00015.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00016.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00017.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00018.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00019.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00020.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00021.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00022.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00023.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00024.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00025.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00026.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00027.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00028.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00029.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00030.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00031.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00032.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00033.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00034.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00035.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00036.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00037.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00038.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00039.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00040.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00041.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00042.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00043.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00044.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00045.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00046.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00047.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00048.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00049.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00050.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00051.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00052.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00053.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00054.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00055.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00056.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00057.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00058.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00059.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00060.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00061.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00062.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00063.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00064.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00065.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00066.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00067.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00068.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00069.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/flow_x_00070.jpg']\n",
      "Total  : 70\n",
      "Train  : 0  (indices 0 … -1)\n",
      "Test   : 68   (indices 0 … 67)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  Build the full dataset (no shuffling here)\n",
    "# ------------------------------------------------------------------\n",
    "data_dir = \"/cs/student/projects1/2021/rstewart/denseflow/build/bull_festival1/\"\n",
    "full_ds  = OpticalFlowDataset(\n",
    "            data_dir,\n",
    "            forecast_offset=1,\n",
    "            sequence_length=sequence_length\n",
    "          )\n",
    "\n",
    "N = len(full_ds)                 # total samples\n",
    "train_ratio = 0               # or whatever you need\n",
    "train_end   = int(train_ratio * N)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  Construct index lists *in order*\n",
    "# ------------------------------------------------------------------\n",
    "train_idx = list(range(train_end))          #  0 … train_end-1\n",
    "test_idx  = list(range(train_end, N - 2))   #  train_end … N-3  (drop last 2)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  Wrap them in Subset objects\n",
    "# ------------------------------------------------------------------\n",
    "train_ds = Subset(full_ds, train_idx)\n",
    "test_ds  = Subset(full_ds, test_idx)\n",
    "\n",
    "print(f\"Total  : {N}\")\n",
    "print(f\"Train  : {len(train_ds)}  (indices 0 … {train_end-1})\")\n",
    "print(f\"Test   : {len(test_ds)}   (indices {train_end} … {N-3})\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  DataLoaders (shuffle only the training loader if desired)\n",
    "# ------------------------------------------------------------------\n",
    "#train_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=1, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator:  fno\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# 3. Instantiate the Model\n",
    "###########################################################################\n",
    "if neural_operator == \"fno\" and sequence_length == 1:\n",
    "    # Our optical flow images have 2 channels. The FNO2d model uses a positional grid.\n",
    "    # When using in_channels=2, the model will add 2 grid channels, resulting in a total of 4 channels\n",
    "    # (which is what we want since our data are 2-channel images).\n",
    "    modes1 = 12  # number of Fourier modes along height\n",
    "    modes2 = 12  # number of Fourier modes along width\n",
    "    hidden_channels = 32\n",
    "\n",
    "    model = FNO2d(\n",
    "        n_modes_height=modes1,\n",
    "        n_modes_width=modes2,\n",
    "        hidden_channels=hidden_channels,\n",
    "        in_channels=2,   # raw optical flow data have 2 channels\n",
    "        out_channels=2   # predicting 2 channels: flow_x and flow_y\n",
    "    )\n",
    "\n",
    "if neural_operator == \"fno\" and sequence_length != 1:\n",
    "    # Our optical flow images have 2 channels. The FNO2d model uses a positional grid.\n",
    "    # When using in_channels=2, the model will add 2 grid channels, resulting in a total of 4 channels\n",
    "    # (which is what we want since our data are 2-channel images).\n",
    "    modes1 = 6  # number of Fourier modes along height\n",
    "    modes2 = 6  # number of Fourier modes along width\n",
    "    hidden_channels = 16\n",
    "    \n",
    "\n",
    "    model = FNO3d(\n",
    "        n_modes_height=modes1,\n",
    "        n_modes_width=modes2,\n",
    "        n_modes_depth=4,\n",
    "        hidden_channels=hidden_channels,\n",
    "        in_channels=2,   # raw optical flow data have 2 channels\n",
    "        out_channels=2   # predicting 2 channels: flow_x and flow_y\n",
    "    )\n",
    "\n",
    "if neural_operator == \"tfno\":\n",
    "    # Our optical flow images have 2 channels. The FNO2d model uses a positional grid.\n",
    "    # When using in_channels=2, the model will add 2 grid channels, resulting in a total of 4 channels\n",
    "    # (which is what we want since our data are 2-channel images).\n",
    "    modes1 = 12  # number of Fourier modes along height\n",
    "    modes2 = 12  # number of Fourier modes along width\n",
    "    hidden_channels = 32\n",
    "\n",
    "    model = TFNO2d(\n",
    "        n_modes_height=modes1,\n",
    "        n_modes_width=modes2,\n",
    "        hidden_channels=hidden_channels,\n",
    "        in_channels=2,   # raw optical flow data have 2 channels\n",
    "        out_channels=2   # predicting 2 channels: flow_x and flow_y\n",
    "    )\n",
    "\n",
    "if neural_operator == \"uno\":\n",
    "    in_channels = 2\n",
    "    out_channels = 2\n",
    "\n",
    "    model = UNO(\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        hidden_channels=16,  # base \"width\"\n",
    "        n_layers=2,          # we have 2 scales in this example\n",
    "\n",
    "        # Multi-scale parameters:\n",
    "        uno_out_channels=[16, 32],\n",
    "        uno_n_modes=[(12, 12), (12, 12)],\n",
    "        # uno_scalings must be list of lists so that uno_scalings[0] is also a list\n",
    "        uno_scalings=[[1, 1], [1, 1]]\n",
    "    )\n",
    "\n",
    "if neural_operator == \"localfno\":\n",
    "    # Example parameters for a 2D local operator with 2 in-channels and 2 out-channels\n",
    "    in_channels = 2   # e.g. optical flow with (flow_x, flow_y)\n",
    "    out_channels = 2  # predicting (flow_x, flow_y) at next time step\n",
    "    kernel_size = 3   # local receptive field size\n",
    "    n_layers = 4      # number of operator layers\n",
    "    hidden_channels = 32\n",
    "    n_dim = 2         # 2D problem\n",
    "    modes1 = 12  # number of Fourier modes along height\n",
    "    modes2 = 12  # number of Fourier modes along width\n",
    "\n",
    "    # Instantiate the LocalNO model\n",
    "    \n",
    "    model = LocalFNO(\n",
    "        n_modes=(modes1, modes2),\n",
    "        default_in_shape=(height, width-800),\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        kernel_size=kernel_size,\n",
    "        n_layers=n_layers,\n",
    "        hidden_channels=hidden_channels,\n",
    "        n_dim=n_dim,\n",
    "        # Optional parameters:\n",
    "        # domain_padding=0,\n",
    "        # domain_padding_mode=\"symmetric\",\n",
    "        # non_linearity=nn.ReLU(),\n",
    "        # dropout=0.0,\n",
    "        # rescaling=None,\n",
    "    )\n",
    "\n",
    "\n",
    "if neural_operator == \"spatiotemporalfno\":\n",
    "     # Optical flow images have 2 channels.\n",
    "    # SpatioTemporalFNO expects input in shape (B, T_in, in_channels, H, W) where T_in is the number of time steps.\n",
    "    # Since you are training on pairs (t, t+1), T_in is set to 2.\n",
    "    modes1 = 12  # Fourier modes along height\n",
    "    modes2 = 12  # Fourier modes along width\n",
    "    hidden_channels = 32\n",
    "    T_in = 3    # Number of input frames (t and t+1)\n",
    "    \n",
    "    model = SpatioTemporalFNO(\n",
    "        n_modes=(modes1, modes2),\n",
    "        in_channels=2,        # raw optical flow has 2 channels\n",
    "        out_channels=2,       # predicting 2 channels: flow_x and flow_y\n",
    "        hidden_channels=hidden_channels,\n",
    "        n_layers=4,           # number of spatio-temporal blocks\n",
    "        T_in=T_in,            # number of input time steps\n",
    "        temporal_kernel=3     # temporal convolution kernel size (default is 3)\n",
    "    )\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(\"Operator: \",neural_operator)\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###########################################################################\n",
    "# 4. Define Loss Function and Optimizer\n",
    "###########################################################################\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] MSE Loss: 0.009979\n",
      "Epoch [2/50] MSE Loss: 0.001167\n",
      "Epoch [3/50] MSE Loss: 0.001011\n",
      "Epoch [4/50] MSE Loss: 0.000794\n",
      "Epoch [5/50] MSE Loss: 0.000617\n",
      "Epoch [6/50] MSE Loss: 0.000524\n",
      "Epoch [7/50] MSE Loss: 0.000458\n",
      "Epoch [8/50] MSE Loss: 0.000378\n",
      "Epoch [9/50] MSE Loss: 0.000401\n",
      "Epoch [10/50] MSE Loss: 0.000400\n",
      "Epoch [11/50] MSE Loss: 0.000432\n",
      "Epoch [12/50] MSE Loss: 0.000351\n",
      "Epoch [13/50] MSE Loss: 0.000343\n",
      "Epoch [14/50] MSE Loss: 0.000334\n",
      "Epoch [15/50] MSE Loss: 0.000337\n",
      "Epoch [16/50] MSE Loss: 0.000286\n",
      "Epoch [17/50] MSE Loss: 0.000307\n",
      "Epoch [18/50] MSE Loss: 0.000255\n",
      "Epoch [19/50] MSE Loss: 0.000252\n",
      "Epoch [20/50] MSE Loss: 0.000250\n",
      "Epoch [21/50] MSE Loss: 0.000224\n",
      "Epoch [22/50] MSE Loss: 0.000198\n",
      "Epoch [23/50] MSE Loss: 0.000286\n",
      "Epoch [24/50] MSE Loss: 0.000178\n",
      "Epoch [25/50] MSE Loss: 0.000188\n",
      "Epoch [26/50] MSE Loss: 0.000231\n",
      "Epoch [27/50] MSE Loss: 0.000170\n",
      "Epoch [28/50] MSE Loss: 0.000161\n",
      "Epoch [29/50] MSE Loss: 0.000165\n",
      "Epoch [30/50] MSE Loss: 0.000172\n",
      "Epoch [31/50] MSE Loss: 0.000154\n",
      "Epoch [32/50] MSE Loss: 0.000173\n",
      "Epoch [33/50] MSE Loss: 0.000178\n",
      "Epoch [34/50] MSE Loss: 0.000280\n",
      "Epoch [35/50] MSE Loss: 0.000149\n",
      "Epoch [36/50] MSE Loss: 0.000164\n",
      "Epoch [37/50] MSE Loss: 0.000158\n",
      "Epoch [38/50] MSE Loss: 0.000164\n",
      "Epoch [39/50] MSE Loss: 0.000144\n",
      "Epoch [40/50] MSE Loss: 0.000145\n",
      "Epoch [41/50] MSE Loss: 0.000150\n",
      "Epoch [42/50] MSE Loss: 0.000170\n",
      "Epoch [43/50] MSE Loss: 0.000159\n",
      "Epoch [44/50] MSE Loss: 0.000162\n",
      "Epoch [45/50] MSE Loss: 0.000162\n",
      "Epoch [46/50] MSE Loss: 0.000166\n",
      "Epoch [47/50] MSE Loss: 0.000163\n",
      "Epoch [48/50] MSE Loss: 0.000154\n",
      "Epoch [49/50] MSE Loss: 0.000159\n",
      "Epoch [50/50] MSE Loss: 0.000164\n",
      "Model saved to localfno_model.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###########################################################################\n",
    "# 5. Training Loop\n",
    "###########################################################################\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.to(device)    # shape: (B, 2, H, W)\n",
    "        targets = targets.to(device)  # shape: (B, 2, H, W)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] MSE Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    # Append the current epoch and average loss to the CSV file.\n",
    "    with open(csv_file, mode=\"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([epoch+1, avg_loss])\n",
    "\n",
    "# Define model save path\n",
    "if sequence_length == 1:\n",
    "    model_save_path = neural_operator+\"_model.pth\"\n",
    "else:\n",
    "    model_save_path = neural_operator+\"_model_\"+str(sequence_length)+\".pth\"\n",
    "\n",
    "# After training is complete\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# inputs has shape (B, T=3, C=2, H, W).\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# We want (B, C=2, depth=3, H, W) for the FNO3d.\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# => (B, 2, 3, H, W)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m       \u001b[38;5;66;03m# add a singleton depth dimension so it becomes (B, 2, 1, H, W)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)               \u001b[38;5;66;03m# (B, 2, 1, H, W)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 5"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# 5. Training Loop\n",
    "###########################################################################\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        # inputs has shape (B, T=3, C=2, H, W).\n",
    "        # We want (B, C=2, depth=3, H, W) for the FNO3d.\n",
    "        inputs = inputs.permute(0, 2, 1, 3, 4)  # => (B, 2, 3, H, W)\n",
    "          # add a singleton depth dimension so it becomes (B, 2, 1, H, W)\n",
    "        targets = targets.unsqueeze(2).to(device)               # (B, 2, 1, H, W)\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)  # now the second dimension is in_channels=2\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] MSE Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    # Append the current epoch and average loss to the CSV file.\n",
    "    with open(csv_file, mode=\"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([epoch+1, avg_loss])\n",
    "\n",
    "# Define model save path\n",
    "if sequence_length == 1:\n",
    "    model_save_path = neural_operator+\"_model.pth\"\n",
    "else:\n",
    "    model_save_path = neural_operator+\"_model_\"+str(sequence_length)+\".pth\"\n",
    "\n",
    "# After training is complete\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_262323/3331128181.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_load_path, map_location=\"cpu\"))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SpatioTemporalFNO:\n\tsize mismatch for lifting_conv.weight: copying a param with shape torch.Size([16, 2, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 2, 3, 3]).\n\tsize mismatch for lifting_conv.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for lifting_residual.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for lifting_residual.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for lifting_residual.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for lifting_residual.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.0.spectral_conv.bias: copying a param with shape torch.Size([16, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 1, 1]).\n\tsize mismatch for blocks.0.spectral_conv.weight.tensor: copying a param with shape torch.Size([16, 16, 6, 4]) from checkpoint, the shape in current model is torch.Size([32, 32, 12, 7]).\n\tsize mismatch for blocks.0.spatial_conv.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.0.spatial_conv.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.0.spatial_conv.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.0.spatial_conv.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.0.temporal_conv.weight: copying a param with shape torch.Size([16, 16, 3, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 1, 1]).\n\tsize mismatch for blocks.0.temporal_conv.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.1.spectral_conv.bias: copying a param with shape torch.Size([16, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 1, 1]).\n\tsize mismatch for blocks.1.spectral_conv.weight.tensor: copying a param with shape torch.Size([16, 16, 6, 4]) from checkpoint, the shape in current model is torch.Size([32, 32, 12, 7]).\n\tsize mismatch for blocks.1.spatial_conv.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.1.spatial_conv.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.1.spatial_conv.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.1.spatial_conv.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.1.temporal_conv.weight: copying a param with shape torch.Size([16, 16, 3, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 1, 1]).\n\tsize mismatch for blocks.1.temporal_conv.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.2.spectral_conv.bias: copying a param with shape torch.Size([16, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 1, 1]).\n\tsize mismatch for blocks.2.spectral_conv.weight.tensor: copying a param with shape torch.Size([16, 16, 6, 4]) from checkpoint, the shape in current model is torch.Size([32, 32, 12, 7]).\n\tsize mismatch for blocks.2.spatial_conv.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.2.spatial_conv.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.2.spatial_conv.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.2.spatial_conv.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.2.temporal_conv.weight: copying a param with shape torch.Size([16, 16, 3, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 1, 1]).\n\tsize mismatch for blocks.2.temporal_conv.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.3.spectral_conv.bias: copying a param with shape torch.Size([16, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 1, 1]).\n\tsize mismatch for blocks.3.spectral_conv.weight.tensor: copying a param with shape torch.Size([16, 16, 6, 4]) from checkpoint, the shape in current model is torch.Size([32, 32, 12, 7]).\n\tsize mismatch for blocks.3.spatial_conv.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.3.spatial_conv.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.3.spatial_conv.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.3.spatial_conv.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.3.temporal_conv.weight: copying a param with shape torch.Size([16, 16, 3, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 1, 1]).\n\tsize mismatch for blocks.3.temporal_conv.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for refinement.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for refinement.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for refinement.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for refinement.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for projection_conv.weight: copying a param with shape torch.Size([2, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([2, 32, 1, 1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[227], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m###########################################################################\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 6. Testing (One-Step Forecasting)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m###########################################################################\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 2) Load the saved weights\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model_load_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/cs/student/projects1/2021/rstewart/code/models/love_parade_3_length/spatiotemporalfno_model_3.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_load_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 3) Move to device, set to eval\u001b[39;00m\n\u001b[1;32m      9\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/cs/student/projects1/2021/rstewart/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SpatioTemporalFNO:\n\tsize mismatch for lifting_conv.weight: copying a param with shape torch.Size([16, 2, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 2, 3, 3]).\n\tsize mismatch for lifting_conv.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for lifting_residual.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for lifting_residual.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for lifting_residual.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for lifting_residual.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.0.spectral_conv.bias: copying a param with shape torch.Size([16, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 1, 1]).\n\tsize mismatch for blocks.0.spectral_conv.weight.tensor: copying a param with shape torch.Size([16, 16, 6, 4]) from checkpoint, the shape in current model is torch.Size([32, 32, 12, 7]).\n\tsize mismatch for blocks.0.spatial_conv.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.0.spatial_conv.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.0.spatial_conv.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.0.spatial_conv.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.0.temporal_conv.weight: copying a param with shape torch.Size([16, 16, 3, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 1, 1]).\n\tsize mismatch for blocks.0.temporal_conv.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.1.spectral_conv.bias: copying a param with shape torch.Size([16, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 1, 1]).\n\tsize mismatch for blocks.1.spectral_conv.weight.tensor: copying a param with shape torch.Size([16, 16, 6, 4]) from checkpoint, the shape in current model is torch.Size([32, 32, 12, 7]).\n\tsize mismatch for blocks.1.spatial_conv.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.1.spatial_conv.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.1.spatial_conv.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.1.spatial_conv.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.1.temporal_conv.weight: copying a param with shape torch.Size([16, 16, 3, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 1, 1]).\n\tsize mismatch for blocks.1.temporal_conv.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.2.spectral_conv.bias: copying a param with shape torch.Size([16, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 1, 1]).\n\tsize mismatch for blocks.2.spectral_conv.weight.tensor: copying a param with shape torch.Size([16, 16, 6, 4]) from checkpoint, the shape in current model is torch.Size([32, 32, 12, 7]).\n\tsize mismatch for blocks.2.spatial_conv.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.2.spatial_conv.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.2.spatial_conv.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.2.spatial_conv.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.2.temporal_conv.weight: copying a param with shape torch.Size([16, 16, 3, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 1, 1]).\n\tsize mismatch for blocks.2.temporal_conv.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.3.spectral_conv.bias: copying a param with shape torch.Size([16, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 1, 1]).\n\tsize mismatch for blocks.3.spectral_conv.weight.tensor: copying a param with shape torch.Size([16, 16, 6, 4]) from checkpoint, the shape in current model is torch.Size([32, 32, 12, 7]).\n\tsize mismatch for blocks.3.spatial_conv.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.3.spatial_conv.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.3.spatial_conv.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.3.spatial_conv.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.3.temporal_conv.weight: copying a param with shape torch.Size([16, 16, 3, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 1, 1]).\n\tsize mismatch for blocks.3.temporal_conv.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for refinement.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for refinement.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for refinement.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for refinement.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for projection_conv.weight: copying a param with shape torch.Size([2, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([2, 32, 1, 1])."
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# 6. Testing (One-Step Forecasting)\n",
    "###########################################################################\n",
    "# 2) Load the saved weights\n",
    "model_load_path = \"/cs/student/projects1/2021/rstewart/code/models/love_parade_3_length/spatiotemporalfno_model_3.pth\"\n",
    "model.load_state_dict(torch.load(model_load_path, map_location=\"cpu\"))\n",
    "\n",
    "# 3) Move to device, set to eval\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for idx, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss_val = criterion(outputs, targets).item()\n",
    "        total_loss += loss_val\n",
    "\n",
    "        print(f\"Test sample {idx} MSE: {loss_val:.6f}\")\n",
    "\n",
    "# Calculate average test loss (MSE)\n",
    "avg_loss = total_loss / len(test_loader)\n",
    "print(f\"Average test MSE: {avg_loss:.6f}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_262323/189490733.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_load_path, map_location=\"cpu\"))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SpatioTemporalFNO:\n\tsize mismatch for lifting_conv.weight: copying a param with shape torch.Size([16, 2, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 2, 3, 3]).\n\tsize mismatch for lifting_conv.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for lifting_residual.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for lifting_residual.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for lifting_residual.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for lifting_residual.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.0.spectral_conv.bias: copying a param with shape torch.Size([16, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 1, 1]).\n\tsize mismatch for blocks.0.spectral_conv.weight.tensor: copying a param with shape torch.Size([16, 16, 6, 4]) from checkpoint, the shape in current model is torch.Size([32, 32, 12, 7]).\n\tsize mismatch for blocks.0.spatial_conv.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.0.spatial_conv.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.0.spatial_conv.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.0.spatial_conv.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.0.temporal_conv.weight: copying a param with shape torch.Size([16, 16, 3, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 1, 1]).\n\tsize mismatch for blocks.0.temporal_conv.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.1.spectral_conv.bias: copying a param with shape torch.Size([16, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 1, 1]).\n\tsize mismatch for blocks.1.spectral_conv.weight.tensor: copying a param with shape torch.Size([16, 16, 6, 4]) from checkpoint, the shape in current model is torch.Size([32, 32, 12, 7]).\n\tsize mismatch for blocks.1.spatial_conv.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.1.spatial_conv.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.1.spatial_conv.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.1.spatial_conv.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.1.temporal_conv.weight: copying a param with shape torch.Size([16, 16, 3, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 1, 1]).\n\tsize mismatch for blocks.1.temporal_conv.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.2.spectral_conv.bias: copying a param with shape torch.Size([16, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 1, 1]).\n\tsize mismatch for blocks.2.spectral_conv.weight.tensor: copying a param with shape torch.Size([16, 16, 6, 4]) from checkpoint, the shape in current model is torch.Size([32, 32, 12, 7]).\n\tsize mismatch for blocks.2.spatial_conv.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.2.spatial_conv.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.2.spatial_conv.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.2.spatial_conv.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.2.temporal_conv.weight: copying a param with shape torch.Size([16, 16, 3, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 1, 1]).\n\tsize mismatch for blocks.2.temporal_conv.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.3.spectral_conv.bias: copying a param with shape torch.Size([16, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 1, 1]).\n\tsize mismatch for blocks.3.spectral_conv.weight.tensor: copying a param with shape torch.Size([16, 16, 6, 4]) from checkpoint, the shape in current model is torch.Size([32, 32, 12, 7]).\n\tsize mismatch for blocks.3.spatial_conv.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.3.spatial_conv.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.3.spatial_conv.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.3.spatial_conv.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.3.temporal_conv.weight: copying a param with shape torch.Size([16, 16, 3, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 1, 1]).\n\tsize mismatch for blocks.3.temporal_conv.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for refinement.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for refinement.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for refinement.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for refinement.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for projection_conv.weight: copying a param with shape torch.Size([2, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([2, 32, 1, 1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[228], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_load_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/cs/student/projects1/2021/rstewart/code/models/love_parade_3_length/spatiotemporalfno_model_3.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_load_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 3) Move to device, set to eval\u001b[39;00m\n\u001b[1;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/cs/student/projects1/2021/rstewart/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SpatioTemporalFNO:\n\tsize mismatch for lifting_conv.weight: copying a param with shape torch.Size([16, 2, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 2, 3, 3]).\n\tsize mismatch for lifting_conv.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for lifting_residual.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for lifting_residual.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for lifting_residual.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for lifting_residual.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.0.spectral_conv.bias: copying a param with shape torch.Size([16, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 1, 1]).\n\tsize mismatch for blocks.0.spectral_conv.weight.tensor: copying a param with shape torch.Size([16, 16, 6, 4]) from checkpoint, the shape in current model is torch.Size([32, 32, 12, 7]).\n\tsize mismatch for blocks.0.spatial_conv.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.0.spatial_conv.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.0.spatial_conv.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.0.spatial_conv.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.0.temporal_conv.weight: copying a param with shape torch.Size([16, 16, 3, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 1, 1]).\n\tsize mismatch for blocks.0.temporal_conv.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.1.spectral_conv.bias: copying a param with shape torch.Size([16, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 1, 1]).\n\tsize mismatch for blocks.1.spectral_conv.weight.tensor: copying a param with shape torch.Size([16, 16, 6, 4]) from checkpoint, the shape in current model is torch.Size([32, 32, 12, 7]).\n\tsize mismatch for blocks.1.spatial_conv.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.1.spatial_conv.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.1.spatial_conv.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.1.spatial_conv.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.1.temporal_conv.weight: copying a param with shape torch.Size([16, 16, 3, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 1, 1]).\n\tsize mismatch for blocks.1.temporal_conv.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.2.spectral_conv.bias: copying a param with shape torch.Size([16, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 1, 1]).\n\tsize mismatch for blocks.2.spectral_conv.weight.tensor: copying a param with shape torch.Size([16, 16, 6, 4]) from checkpoint, the shape in current model is torch.Size([32, 32, 12, 7]).\n\tsize mismatch for blocks.2.spatial_conv.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.2.spatial_conv.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.2.spatial_conv.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.2.spatial_conv.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.2.temporal_conv.weight: copying a param with shape torch.Size([16, 16, 3, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 1, 1]).\n\tsize mismatch for blocks.2.temporal_conv.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.3.spectral_conv.bias: copying a param with shape torch.Size([16, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 1, 1]).\n\tsize mismatch for blocks.3.spectral_conv.weight.tensor: copying a param with shape torch.Size([16, 16, 6, 4]) from checkpoint, the shape in current model is torch.Size([32, 32, 12, 7]).\n\tsize mismatch for blocks.3.spatial_conv.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.3.spatial_conv.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.3.spatial_conv.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for blocks.3.spatial_conv.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for blocks.3.temporal_conv.weight: copying a param with shape torch.Size([16, 16, 3, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 1, 1]).\n\tsize mismatch for blocks.3.temporal_conv.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for refinement.0.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for refinement.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for refinement.2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for refinement.2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for projection_conv.weight: copying a param with shape torch.Size([2, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([2, 32, 1, 1])."
     ]
    }
   ],
   "source": [
    "model_load_path = \"/cs/student/projects1/2021/rstewart/code/models/bull1_3_length/spatiotemporalfno_model_3.pth\"\n",
    "model.load_state_dict(torch.load(model_load_path, map_location=\"cpu\"))\n",
    "\n",
    "# 3) Move to device, set to eval\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "total_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (inputs, targets) in enumerate(test_loader):\n",
    "        # 1 Arrange shapes exactly like training\n",
    "        inputs  = inputs.permute(0, 2, 1, 3, 4).to(device)   # (B, 2, 3, H, W)\n",
    "        targets = targets.unsqueeze(2).to(device)            # (B, 2, 1, H, W)\n",
    "\n",
    "        # 2 Forward pass\n",
    "        outputs = model(inputs)                              # (B, 2, 3, H, W)\n",
    "\n",
    "        # 3 Same loss definition as training\n",
    "        loss_val = criterion(outputs, targets).item()\n",
    "        total_loss += loss_val\n",
    "        print(f\"Test sample {idx:02d} – MSE: {loss_val:.6f}\")\n",
    "\n",
    "avg_loss = total_loss / len(test_loader)\n",
    "print(f\"Average test MSE: {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_fno = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_stfno = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_262323/1468481589.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_load_path, map_location=\"cpu\"))\n",
      "/cs/student/projects1/2021/rstewart/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1, 2, 1, 966, 1786])) that is different to the input size (torch.Size([1, 2, 3, 966, 1786])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "criterion = MSELoss(reduction=\"none\")  # so we can compute per-sample losses\n",
    "model_load_path = \"/cs/student/projects1/2021/rstewart/code/models/bull1_1_length/fno_model.pth\"\n",
    "model.load_state_dict(torch.load(model_load_path, map_location=\"cpu\"))\n",
    "\n",
    "# 3) Move to device, set to eval\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs  = inputs.permute(0, 2, 1, 3, 4).to(device)   # (B, 2, 3, H, W)\n",
    "        targets = targets.unsqueeze(2).to(device)            # (B, 2, 1, H, W)\n",
    "\n",
    "        out_fno = model(inputs)\n",
    "        \n",
    "\n",
    "        # per-sample MSE (averaged over channels, H, W)\n",
    "        # criterion gives [B, C, H, W]; we mean over (1,2,3)\n",
    "        per_sample = criterion(out_fno, targets).mean(dim=(1, 2, 3, 4)) \n",
    "        mse_fno.append(per_sample.cpu().numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_262323/3773782345.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_load_path, map_location=\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "criterion = MSELoss(reduction=\"none\")  # so we can compute per-sample losses\n",
    "model_load_path = \"/cs/student/projects1/2021/rstewart/code/models/bull1_1_length/fno_model.pth\"\n",
    "model.load_state_dict(torch.load(model_load_path, map_location=\"cpu\"))\n",
    "\n",
    "# 3) Move to device, set to eval\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        out_fno = model(inputs)\n",
    "        \n",
    "\n",
    "        # per-sample MSE (averaged over channels, H, W)\n",
    "        # criterion gives [B, C, H, W]; we mean over (1,2,3)\n",
    "        mse_fno.append(criterion(out_fno, targets).mean(dim=(1,2,3)).cpu().numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_fno = np.concatenate(mse_fno)   # shape (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_stfno = np.concatenate(mse_stfno)   # shape (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 68\n"
     ]
    }
   ],
   "source": [
    "print(len(mse_fno), len(mse_stfno))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_fno   = np.asarray(mse_fno,   dtype=float).ravel()   # shape (Nf,)\n",
    "mse_stfno = np.asarray(mse_stfno, dtype=float).ravel()   # shape (Ns,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 4.258, one‑sided p = 3.293e-05\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "t_stat, p_one = ttest_rel(mse_fno, mse_stfno, alternative=\"greater\")\n",
    "print(f\"t = {t_stat:.3f}, one‑sided p = {p_one:.3e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 5, 1], expected input[1, 6, 6582960] to have 5 channels, but got 6 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Convert tensors to NumPy arrays.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m input_img  \u001b[38;5;241m=\u001b[39m tensor_to_numpy(inputs)    \u001b[38;5;66;03m# shape: (H, W, 2)\u001b[39;00m\n",
      "File \u001b[0;32m/cs/student/projects1/2021/rstewart/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cs/student/projects1/2021/rstewart/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/cs/student/projects1/2021/rstewart/anaconda3/envs/myenv/lib/python3.9/site-packages/neuralop/models/fno.py:372\u001b[0m, in \u001b[0;36mFNO.forward\u001b[0;34m(self, x, output_shape, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding(x)\n\u001b[0;32m--> 372\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlifting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain_padding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain_padding\u001b[38;5;241m.\u001b[39mpad(x)\n",
      "File \u001b[0;32m/cs/student/projects1/2021/rstewart/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cs/student/projects1/2021/rstewart/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/cs/student/projects1/2021/rstewart/anaconda3/envs/myenv/lib/python3.9/site-packages/neuralop/layers/channel_mlp.py:73\u001b[0m, in \u001b[0;36mChannelMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m     reshaped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, fc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfcs):\n\u001b[0;32m---> 73\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     75\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_linearity(x)\n",
      "File \u001b[0;32m/cs/student/projects1/2021/rstewart/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cs/student/projects1/2021/rstewart/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/cs/student/projects1/2021/rstewart/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cs/student/projects1/2021/rstewart/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[1;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    369\u001b[0m     )\n\u001b[0;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 5, 1], expected input[1, 6, 6582960] to have 5 channels, but got 6 channels instead"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tensor_to_numpy(tensor):\n",
    "    # Ensure the tensor is dense and remove the batch dimension.\n",
    "    if tensor.is_sparse:\n",
    "        tensor = tensor.to_dense()\n",
    "    # Remove the batch dimension (assuming batch size is 1) and change shape from (C, H, W) to (H, W, C)\n",
    "    return tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Convert tensors to NumPy arrays.\n",
    "        input_img  = tensor_to_numpy(inputs)    # shape: (H, W, 2)\n",
    "        target_img = tensor_to_numpy(targets)    # shape: (H, W, 2)\n",
    "        output_img = tensor_to_numpy(outputs)    # shape: (H, W, 2)\n",
    "        \n",
    "        # Compute the difference (target - prediction) for each channel.\n",
    "        diff_img = target_img - output_img\n",
    "\n",
    "        # Create a figure with 4 rows and 2 columns.\n",
    "        fig, axs = plt.subplots(4, 2, figsize=(10, 20))\n",
    "        \n",
    "        # Row 1: Input\n",
    "        axs[0, 0].imshow(input_img[..., 0], cmap='gray')\n",
    "        axs[0, 0].set_title(f\"Sample {idx}: Input Flow X\")\n",
    "        axs[0, 1].imshow(input_img[..., 1], cmap='gray')\n",
    "        axs[0, 1].set_title(f\"Sample {idx}: Input Flow Y\")\n",
    "        \n",
    "        # Row 2: Target (Ground Truth)\n",
    "        axs[1, 0].imshow(target_img[..., 0], cmap='gray')\n",
    "        axs[1, 0].set_title(f\"Sample {idx}: Target Flow X\")\n",
    "        axs[1, 1].imshow(target_img[..., 1], cmap='gray')\n",
    "        axs[1, 1].set_title(f\"Sample {idx}: Target Flow Y\")\n",
    "        \n",
    "        # Row 3: Prediction (Model Output)\n",
    "        axs[2, 0].imshow(output_img[..., 0], cmap='gray')\n",
    "        axs[2, 0].set_title(f\"Sample {idx}: Prediction Flow X\")\n",
    "        axs[2, 1].imshow(output_img[..., 1], cmap='gray')\n",
    "        axs[2, 1].set_title(f\"Sample {idx}: Prediction Flow Y\")\n",
    "        \n",
    "        # Row 4: Difference (Target - Prediction) as heatmaps.\n",
    "        # Using a diverging colormap like 'seismic' to show positive/negative differences.\n",
    "        im0 = axs[3, 0].imshow(diff_img[..., 0], cmap='seismic')\n",
    "        axs[3, 0].set_title(f\"Sample {idx}: Diff Flow X\")\n",
    "        fig.colorbar(im0, ax=axs[3, 0])\n",
    "        \n",
    "        im1 = axs[3, 1].imshow(diff_img[..., 1], cmap='seismic')\n",
    "        axs[3, 1].set_title(f\"Sample {idx}: Diff Flow Y\")\n",
    "        fig.colorbar(im1, ax=axs[3, 1])\n",
    "        \n",
    "        # Remove axis ticks for clarity.\n",
    "        for ax in axs.flat:\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
