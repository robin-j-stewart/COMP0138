{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "from neuralop.models import FNO2d, TFNO2d, UNO, LocalFNO, FNO3d, TFNO3d\n",
    "from spatioTemporalFNO import SpatioTemporalFNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width: 2460 Height: 1338\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00000.jpg\"\n",
    "img = Image.open(image_path)\n",
    "width, height = img.size\n",
    "print(\"Width:\", width, \"Height:\", height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_operator = \"fno\"\n",
    "sequence_length = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSV file and write a header.\n",
    "if sequence_length == 1:\n",
    "    csv_file = \"training_history_\"+neural_operator+\"_bull2.csv\"\n",
    "else:\n",
    "    csv_file = \"training_history_\"+neural_operator+\"_bull2_\"+str(sequence_length)+\".csv\"\n",
    "with open(csv_file, mode=\"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Epoch\", \"MSE Loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class OpticalFlowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads optical flow images for forecasting.\n",
    "    \n",
    "    This dataset can operate in two modes:\n",
    "      - Original Mode (single-frame input): when sequence_length == 1.\n",
    "        Returns:\n",
    "           input  = optical flow at time t (shape: (C, H, W))\n",
    "           target = optical flow at time t + forecast_offset (shape: (C, H, W))\n",
    "      - Sequence Mode (multi-frame input): when sequence_length > 1.\n",
    "        Returns:\n",
    "           input  = sequence of optical flow frames of length `sequence_length` \n",
    "                    (shape: (T, C, H, W))\n",
    "           target = optical flow at time t + sequence_length + forecast_offset - 1\n",
    "                    (shape: (C, H, W))\n",
    "                    \n",
    "    The dataset length is computed appropriately based on the mode.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, transform=None, sequence_length=1, forecast_offset=1):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.sequence_length = sequence_length  # Set to 1 for original behavior; >1 for multi-frame input.\n",
    "        self.forecast_offset = forecast_offset\n",
    "        \n",
    "        self.samples = []\n",
    "        flow_x_files = sorted(glob.glob(os.path.join(data_dir, 'flow_x_*.jpg')))\n",
    "        print(\"Found flow_x files:\", flow_x_files)\n",
    "        for fx in flow_x_files:\n",
    "            fy = fx.replace(\"flow_x_\", \"flow_y_\")\n",
    "            if os.path.exists(fy):\n",
    "                self.samples.append((fx, fy))\n",
    "            else:\n",
    "                print(f\"Warning: Matching flow_y image not found for {fx}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.sequence_length == 1:\n",
    "            # Original: need one frame for input plus forecast_offset for target.\n",
    "            return len(self.samples) - self.forecast_offset\n",
    "        else:\n",
    "            # Sequence mode: need sequence_length frames for input plus forecast_offset.\n",
    "            return len(self.samples) - (self.sequence_length - 1 + self.forecast_offset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.sequence_length == 1:\n",
    "            # ----- Original Mode -----\n",
    "            # Input frame at time t\n",
    "            flow_x_file, flow_y_file = self.samples[idx]\n",
    "            input_flow_x = np.array(Image.open(flow_x_file).convert('L'), dtype=np.float32) / 255.0\n",
    "            input_flow_y = np.array(Image.open(flow_y_file).convert('L'), dtype=np.float32) / 255.0\n",
    "            input_flow = np.stack([input_flow_x, input_flow_y], axis=-1)  # (H, W, 2)\n",
    "            \n",
    "            # Target frame at time t + forecast_offset\n",
    "            target_flow_x_file, target_flow_y_file = self.samples[idx + self.forecast_offset]\n",
    "            target_flow_x = np.array(Image.open(target_flow_x_file).convert('L'), dtype=np.float32) / 255.0\n",
    "            target_flow_y = np.array(Image.open(target_flow_y_file).convert('L'), dtype=np.float32) / 255.0\n",
    "            target_flow = np.stack([target_flow_x, target_flow_y], axis=-1)\n",
    "            \n",
    "            if self.transform is not None:\n",
    "                input_flow = self.transform(input_flow)\n",
    "                target_flow = self.transform(target_flow)\n",
    "            \n",
    "            # Convert to tensor and change order to (C, H, W)\n",
    "            input_tensor = torch.tensor(input_flow, dtype=torch.float32).permute(2, 0, 1)\n",
    "            target_tensor = torch.tensor(target_flow, dtype=torch.float32).permute(2, 0, 1)\n",
    "            \n",
    "            return input_tensor, target_tensor\n",
    "        \n",
    "        else:\n",
    "            # ----- Sequence Mode -----\n",
    "            # Build the input sequence from idx to idx + sequence_length - 1\n",
    "            input_frames = []\n",
    "            for i in range(self.sequence_length):\n",
    "                flow_x_file, flow_y_file = self.samples[idx + i]\n",
    "                input_flow_x = np.array(Image.open(flow_x_file).convert('L'), dtype=np.float32) / 255.0\n",
    "                input_flow_y = np.array(Image.open(flow_y_file).convert('L'), dtype=np.float32) / 255.0\n",
    "                flow = np.stack([input_flow_x, input_flow_y], axis=-1)  # (H, W, 2)\n",
    "                if self.transform is not None:\n",
    "                    flow = self.transform(flow)\n",
    "                flow_tensor = torch.tensor(flow, dtype=torch.float32).permute(2, 0, 1)\n",
    "                input_frames.append(flow_tensor)\n",
    "            # Stack the sequence into a tensor: (T, C, H, W)\n",
    "            input_tensor = torch.stack(input_frames, dim=0)\n",
    "            \n",
    "            # Target frame: located at index = idx + sequence_length + forecast_offset - 1\n",
    "            target_idx = idx + self.sequence_length + self.forecast_offset - 1\n",
    "            target_flow_x_file, target_flow_y_file = self.samples[target_idx]\n",
    "            target_flow_x = np.array(Image.open(target_flow_x_file).convert('L'), dtype=np.float32) / 255.0\n",
    "            target_flow_y = np.array(Image.open(target_flow_y_file).convert('L'), dtype=np.float32) / 255.0\n",
    "            target_flow = np.stack([target_flow_x, target_flow_y], axis=-1)\n",
    "            if self.transform is not None:\n",
    "                target_flow = self.transform(target_flow)\n",
    "            target_tensor = torch.tensor(target_flow, dtype=torch.float32).permute(2, 0, 1)\n",
    "            \n",
    "            return input_tensor, target_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found flow_x files: ['/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00000.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00001.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00003.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00004.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00005.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00006.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00008.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00010.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00011.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00013.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00015.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00017.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00018.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00019.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00020.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00022.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00023.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00024.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00025.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00026.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00027.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00028.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00029.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00030.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00031.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00034.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00035.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00036.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00037.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00039.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00040.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00041.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00042.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00043.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00044.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00045.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00046.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00047.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00048.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00049.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00050.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00051.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00052.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00054.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00055.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00056.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00057.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00058.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00059.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00060.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00061.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00062.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00063.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00064.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00065.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00066.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00067.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00068.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00069.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00070.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00071.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00072.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00073.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00074.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00075.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00077.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00078.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00080.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00082.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00083.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00085.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00087.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00088.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00089.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00090.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00091.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00092.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00094.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00095.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00096.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00097.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00098.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00099.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00101.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00102.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00104.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00106.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00107.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00108.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00109.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00110.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00111.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00112.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00113.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00114.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00115.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00117.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00118.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00120.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00122.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00123.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00124.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00125.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00126.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00127.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00128.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00129.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00130.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00132.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00133.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00134.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00135.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00136.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00137.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00138.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00139.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00140.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00141.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00142.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00143.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00144.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00145.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00147.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00148.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00149.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00150.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00152.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00154.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00155.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00157.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00159.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00161.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00162.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00163.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00164.jpg', '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/flow_x_00165.jpg']\n",
      "Total available samples for forecasting: 135\n",
      "Train dataset size: 108\n",
      "Test dataset size: 27\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "data_dir = '/cs/student/projects1/2021/rstewart/denseflow/build/love_parade/' \n",
    "\n",
    "# Create dataset with forecast_offset=1 (predicting next frame)\n",
    "dataset = OpticalFlowDataset(data_dir, forecast_offset=1, sequence_length=sequence_length)\n",
    "print(\"Total available samples for forecasting:\", len(dataset)) \n",
    "\n",
    "# Randomly split the dataset into training and testing\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Test dataset size:\", len(test_dataset))\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 1  # Adjust as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator:  fno\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if neural_operator == \"fno\" and sequence_length == 1:\n",
    "    modes1 = 12  \n",
    "    modes2 = 12  \n",
    "    hidden_channels = 32\n",
    "\n",
    "    model = FNO2d(\n",
    "        n_modes_height=modes1,\n",
    "        n_modes_width=modes2,\n",
    "        hidden_channels=hidden_channels,\n",
    "        in_channels=2,   \n",
    "        out_channels=2  \n",
    "    )\n",
    "\n",
    "if neural_operator == \"fno\" and sequence_length != 1:\n",
    "    modes1 = 12\n",
    "    modes2 = 12  \n",
    "    hidden_channels = 32\n",
    "    \n",
    "    model = FNO3d(\n",
    "        n_modes_height=modes1,\n",
    "        n_modes_width=modes2,\n",
    "        n_modes_depth=4,\n",
    "        hidden_channels=hidden_channels,\n",
    "        in_channels=2,   \n",
    "        out_channels=2  \n",
    "    )\n",
    "\n",
    "if neural_operator == \"tfno\":\n",
    "    modes1 = 12  \n",
    "    modes2 = 12  \n",
    "    hidden_channels = 32\n",
    "\n",
    "    model = TFNO2d(\n",
    "        n_modes_height=modes1,\n",
    "        n_modes_width=modes2,\n",
    "        hidden_channels=hidden_channels,\n",
    "        in_channels=2,   \n",
    "        out_channels=2  \n",
    "    )\n",
    "\n",
    "if neural_operator == \"uno\":\n",
    "    in_channels = 2\n",
    "    out_channels = 2\n",
    "\n",
    "    model = UNO(\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        hidden_channels=32,  \n",
    "        n_layers=2,         \n",
    "        uno_out_channels=[32, 64],\n",
    "        uno_n_modes=[(12, 12), (12, 12)],\n",
    "        uno_scalings=[[1, 1], [1, 1]]\n",
    "    )\n",
    "\n",
    "if neural_operator == \"localfno\":\n",
    "    in_channels = 2   \n",
    "    out_channels = 2  \n",
    "    kernel_size = 3   \n",
    "    n_layers = 4      \n",
    "    hidden_channels = 32\n",
    "    n_dim = 2         \n",
    "    modes1 = 12  \n",
    "    modes2 = 12  \n",
    "    \n",
    "    model = LocalFNO(\n",
    "        n_modes=(modes1, modes2),\n",
    "        default_in_shape=(height, width),\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        kernel_size=kernel_size,\n",
    "        n_layers=n_layers,\n",
    "        hidden_channels=hidden_channels,\n",
    "        n_dim=n_dim,\n",
    "    )\n",
    "\n",
    "if neural_operator == \"spatiotemporalfno\":\n",
    "    modes1 = 12  \n",
    "    modes2 = 12 \n",
    "    hidden_channels = 32\n",
    "    T_in = 3   \n",
    "    \n",
    "    model = SpatioTemporalFNO(\n",
    "        n_modes=(modes1, modes2),\n",
    "        in_channels=2,       \n",
    "        out_channels=2,       \n",
    "        hidden_channels=hidden_channels,\n",
    "        n_layers=4,           \n",
    "        T_in=T_in,            \n",
    "        temporal_kernel=3    \n",
    "    )\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(\"Operator: \",neural_operator)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.to(device)  \n",
    "        targets = targets.to(device)  \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] MSE Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    with open(csv_file, mode=\"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([epoch+1, avg_loss])\n",
    "\n",
    "if sequence_length == 1:\n",
    "    model_save_path = neural_operator+\"_model.pth\"\n",
    "else:\n",
    "    model_save_path = neural_operator+\"_model_\"+str(sequence_length)+\".pth\"\n",
    "\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# 5. Training Loop\n",
    "###########################################################################\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        # inputs has shape (B, T=3, C=2, H, W).\n",
    "        # We want (B, C=2, depth=3, H, W) for the FNO3d.\n",
    "        inputs = inputs.permute(0, 2, 1, 3, 4)  # => (B, 2, 3, H, W)\n",
    "          # add a singleton depth dimension so it becomes (B, 2, 1, H, W)\n",
    "        targets = targets.unsqueeze(2).to(device)               # (B, 2, 1, H, W)\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)  # now the second dimension is in_channels=2\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] MSE Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    # Append the current epoch and average loss to the CSV file.\n",
    "    with open(csv_file, mode=\"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([epoch+1, avg_loss])\n",
    "\n",
    "# Define model save path\n",
    "if sequence_length == 1:\n",
    "    model_save_path = neural_operator+\"_model.pth\"\n",
    "else:\n",
    "    model_save_path = neural_operator+\"_model_\"+str(sequence_length)+\".pth\"\n",
    "\n",
    "# After training is complete\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load_path = \"/cs/student/projects1/2021/rstewart/code/models/love_parade_3_length/spatiotemporalfno_model_3.pth\"\n",
    "model.load_state_dict(torch.load(model_load_path, map_location=\"cpu\"))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for idx, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss_val = criterion(outputs, targets).item()\n",
    "        total_loss += loss_val\n",
    "\n",
    "        print(f\"Test sample {idx} MSE: {loss_val:.6f}\")\n",
    "\n",
    "avg_loss = total_loss / len(test_loader)\n",
    "print(f\"Average test MSE: {avg_loss:.6f}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_fno = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_stfno = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "criterion = MSELoss(reduction=\"none\")  # so we can compute per-sample losses\n",
    "model_load_path = \"/cs/student/projects1/2021/rstewart/code/models/bull1_1_length/fno_model.pth\"\n",
    "model.load_state_dict(torch.load(model_load_path, map_location=\"cpu\"))\n",
    "\n",
    "# 3) Move to device, set to eval\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        out_fno = model(inputs)\n",
    "        \n",
    "\n",
    "        # per-sample MSE (averaged over channels, H, W)\n",
    "        # criterion gives [B, C, H, W]; we mean over (1,2,3)\n",
    "        mse_fno.append(criterion(out_fno, targets).mean(dim=(1,2,3)).cpu().numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_fno = np.concatenate(mse_fno)   # shape (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_stfno = np.concatenate(mse_stfno)   # shape (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "t_stat, p_one = ttest_rel(mse_fno, mse_stfno, alternative=\"greater\")\n",
    "print(f\"t = {t_stat:.3f}, one‑sided p = {p_one:.3e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizations will be saved to: visualisations/love_parade_3_length/fno_visualisations\n",
      "Loading model from: /cs/student/projects1/2021/rstewart/code/models/love_parade_3_length/fno_model_3.pth to device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2732452/829392219.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_load_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined and individual visualizations saved to subdirectories within 'visualisations/love_parade_3_length/fno_visualisations'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image \n",
    "import sys \n",
    "\n",
    "def tensor_to_numpy(tensor, is_input_sequence=False):\n",
    "    if tensor.is_sparse:\n",
    "        tensor = tensor.to_dense()\n",
    "\n",
    "    # Assuming batch_size is 1 for visualization loop\n",
    "    # Remove batch dim: shape becomes (S, C, H, W) OR (C, S_out, H, W) OR (C, H, W)\n",
    "    tensor = tensor.squeeze(0)\n",
    "\n",
    "    if is_input_sequence:\n",
    "        # Input sequence: Expected (S, C, H, W) -> permute to (S, H, W, C)\n",
    "        if tensor.ndim != 4:\n",
    "             print(f\"Warning: Expected input sequence tensor shape (S, C, H, W) after batch squeeze, but got {tensor.shape}\")\n",
    "             # Attempt to proceed, but permutation might fail\n",
    "        return tensor.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    else:\n",
    "        # Target/Output: Expected (C, H, W) or (C, 1, H, W) or (C, S_out, H, W) but we only want (C, H, W)\n",
    "        # If it has 4 dimensions, we assume it's (C, S_out, H, W) and we should only process the first frame (S_out=0)\n",
    "        if tensor.ndim == 4:\n",
    "             if tensor.shape[1] == 1:\n",
    "                 tensor = tensor.squeeze(1) \n",
    "             else:\n",
    "                 print(f\"Error: Non-input tensor has unexpected shape (C, S_out, H, W) with S_out > 1 after batch squeeze: {tensor.shape}. This function expects (C, H, W) or (C, 1, H, W) for non-inputs.\")\n",
    "\n",
    "        if tensor.ndim == 3:\n",
    "             return tensor.permute(1, 2, 0).cpu().numpy()\n",
    "        else:\n",
    "             print(f\"Error: Final non-input tensor shape not (C, H, W) for permute: {tensor.shape}\")\n",
    "             # Fallback: Return the tensor as is after CPU transfer, without the expected (H, W, C) shape\n",
    "             return tensor.cpu().numpy()\n",
    "\n",
    "\n",
    "# Define the model loading path\n",
    "model_load_path = \"/cs/student/projects1/2021/rstewart/code/models/love_parade_3_length/fno_model_3.pth\"\n",
    "model_folder_name = os.path.basename(os.path.dirname(model_load_path))\n",
    "filename_stem = os.path.splitext(os.path.basename(model_load_path))[0]\n",
    "model_type_name = filename_stem.split('_model')[0] if '_model' in filename_stem else filename_stem\n",
    "base_viz_dir = \"visualisations\"\n",
    "model_viz_subdir = f\"{model_type_name}_visualisations\"\n",
    "main_output_dir = os.path.join(base_viz_dir, model_folder_name, model_viz_subdir)\n",
    "\n",
    "os.makedirs(main_output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Visualizations will be saved to: {main_output_dir}\")\n",
    "\n",
    "try:\n",
    "    print(f\"Loading model from: {model_load_path} to device: {device}\")\n",
    "    model.load_state_dict(torch.load(model_load_path, map_location=device))\n",
    "    model.to(device) \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            inputs_for_model = inputs.permute(0, 2, 1, 3, 4) \n",
    "            outputs = model(inputs_for_model)\n",
    "\n",
    "            input_imgs  = tensor_to_numpy(inputs, is_input_sequence=True)\n",
    "            target_img = tensor_to_numpy(targets, is_input_sequence=False)\n",
    "            first_predicted_frame = outputs[:, :, 0:1, :, :] \n",
    "\n",
    "            output_img = tensor_to_numpy(first_predicted_frame, is_input_sequence=False)\n",
    "\n",
    "\n",
    "            # Compute the difference (target - prediction) for each channel.\n",
    "            # This now works as both target_img and output_img are (H, W, 2)\n",
    "            diff_img = target_img - output_img\n",
    "\n",
    "            # Create a sub-directory for the current sample inside the main output directory\n",
    "            sample_output_dir = os.path.join(main_output_dir, f\"sample_{idx}\")\n",
    "            os.makedirs(sample_output_dir, exist_ok=True)\n",
    "\n",
    "            # Determine the number of input frames\n",
    "            S_in = input_imgs.shape[0]\n",
    "            # Total rows needed: Input frames + Target + Pred + Diff\n",
    "            total_rows_needed = S_in + 3\n",
    "\n",
    "            fig, axs = plt.subplots(total_rows_needed, 2, figsize=(10, 4 * total_rows_needed)) \n",
    "            for i in range(S_in):\n",
    "                axs[i, 0].imshow(input_imgs[i, ..., 0], cmap='gray')\n",
    "                axs[i, 0].set_title(f\"Sample {idx}: Input Frame {i} Flow X\")\n",
    "                axs[i, 1].imshow(input_imgs[i, ..., 1], cmap='gray')\n",
    "                axs[i, 1].set_title(f\"Sample {idx}: Input Frame {i} Flow Y\")\n",
    "\n",
    "            target_row = S_in\n",
    "            axs[target_row, 0].imshow(target_img[..., 0], cmap='gray')\n",
    "            axs[target_row, 0].set_title(f\"Sample {idx}: Target Flow X\")\n",
    "            axs[target_row, 1].imshow(target_img[..., 1], cmap='gray')\n",
    "            axs[target_row, 1].set_title(f\"Sample {idx}: Target Flow Y\")\n",
    "\n",
    "            pred_row = target_row + 1\n",
    "            axs[pred_row, 0].imshow(output_img[..., 0], cmap='gray')\n",
    "            axs[pred_row, 0].set_title(f\"Sample {idx}: Prediction (Frame 0) Flow X\") \n",
    "            axs[pred_row, 1].imshow(output_img[..., 1], cmap='gray')\n",
    "            axs[pred_row, 1].set_title(f\"Sample {idx}: Prediction (Frame 0) Flow Y\") \n",
    "\n",
    "            diff_row = pred_row + 1\n",
    "            im0 = axs[diff_row, 0].imshow(diff_img[..., 0])\n",
    "            axs[diff_row, 0].set_title(f\"Sample {idx}: Diff (Target - Pred Frame 0) Flow X\") \n",
    "            fig.colorbar(im0, ax=axs[diff_row, 0])\n",
    "\n",
    "            im1 = axs[diff_row, 1].imshow(diff_img[..., 1],)\n",
    "            axs[diff_row, 1].set_title(f\"Sample {idx}: Diff (Target - Pred Frame 0) Flow Y\") \n",
    "            fig.colorbar(im1, ax=axs[diff_row, 1])\n",
    "\n",
    "            for ax in axs.flat:\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "\n",
    "            plt.tight_layout()\n",
    "            fig.canvas.draw() \n",
    "\n",
    "            axes_info = []\n",
    "            for i in range(S_in): # Input frames\n",
    "                axes_info.append((axs[i, 0], f'input_frame_{i}_flow_x.png'))\n",
    "                axes_info.append((axs[i, 1], f'input_frame_{i}_flow_y.png'))\n",
    "\n",
    "            # Target, Prediction (Frame 0), Difference - Use calculated row indices\n",
    "            axes_info.extend([\n",
    "                (axs[target_row, 0], 'target_flow_x.png'),\n",
    "                (axs[target_row, 1], 'target_flow_y.png'),\n",
    "                (axs[pred_row, 0], 'prediction_frame_0_flow_x.png'),\n",
    "                (axs[pred_row, 1], 'prediction_frame_0_flow_y.png'), \n",
    "                (axs[diff_row, 0], 'diff_target_pred_frame_0_flow_x.png'),\n",
    "                (axs[diff_row, 1], 'diff_target_pred_frame_0_flow_y.png'),\n",
    "            ])\n",
    "\n",
    "\n",
    "            # Get the bounding boxes of each axis in display (pixel) coordinates\n",
    "            bbox_list = [ax.get_tightbbox(fig.canvas.get_renderer()) for ax, _ in axes_info]\n",
    "\n",
    "            # Define the path for the combined image within the sample directory\n",
    "            combined_save_path = os.path.join(sample_output_dir, \"combined_visualization.png\")\n",
    "\n",
    "            try:\n",
    "                 fig.savefig(combined_save_path, dpi=fig.dpi) \n",
    "            except Exception as save_err:\n",
    "                 print(f\"Error saving combined figure for sample {idx}: {save_err}\")\n",
    "\n",
    "\n",
    "            plt.close(fig)\n",
    "\n",
    "            combined_img = None \n",
    "            try:\n",
    "                if os.path.exists(combined_save_path): \n",
    "                    combined_img = Image.open(combined_save_path)\n",
    "                    img_width, img_height = combined_img.size\n",
    "\n",
    "                    for ((ax_info, filename), bbox) in zip(axes_info, bbox_list):\n",
    "                        [[x0, y0], [x1, y1]] = bbox.get_points()\n",
    "\n",
    "                        left = int(round(x0))\n",
    "                        upper = int(round(img_height - y1)) \n",
    "                        right = int(round(x1))\n",
    "                        lower = int(round(img_height - y0)) \n",
    "\n",
    "                        left = max(0, left)\n",
    "                        upper = max(0, upper)\n",
    "                        right = min(img_width, right)\n",
    "                        lower = min(img_height, lower)\n",
    "\n",
    "                        # Ensure bounding box is valid before cropping\n",
    "                        if right > left and lower > upper:\n",
    "                            cropped_img = combined_img.crop((left, upper, right, lower))\n",
    "                            individual_save_path = os.path.join(sample_output_dir, filename)\n",
    "                            cropped_img.save(individual_save_path)\n",
    "                            cropped_img.close()\n",
    "                        else:\n",
    "                            print(f\"Warning: Invalid bounding box [{left},{upper},{right},{lower}] for sample {idx}, file {filename} (img size: {img_width}x{img_height}). Skipping crop.\")\n",
    "\n",
    "\n",
    "            except Exception as crop_err:\n",
    "                 print(f\"Error processing or cropping combined image for sample {idx}: {crop_err}\")\n",
    "                 import traceback\n",
    "                 traceback.print_exc()\n",
    "            finally:\n",
    "                if combined_img:\n",
    "                    combined_img.close()\n",
    "\n",
    "\n",
    "    print(f\"Combined and individual visualizations saved to subdirectories within '{main_output_dir}'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Catch broader exceptions including OSError for environment issues or other errors\n",
    "    print(f\"An error occurred during processing: {e}\")\n",
    "    print(\"Please check the error message and ensure your environment is correctly set up (e.g., CUDA, dependencies).\")\n",
    "    import traceback\n",
    "    traceback.print_exc() # Print detailed traceback"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
